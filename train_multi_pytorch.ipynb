{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aift-ml/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-05-03 14:16:45.460512: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-03 14:16:45.590238: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-05-03 14:16:46.106719: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2023-05-03 14:16:46.106773: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2023-05-03 14:16:46.106778: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import ClassLabel, Dataset, DatasetDict, Features, Value, Sequence\n",
    "import pyarrow as pa\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import torch ## version >= 1.8.2\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl ## version == 1.4.9\n",
    "import datasets ## version == 2.1.0\n",
    "from transformers import AutoTokenizer, AutoModel ## version == 4.12.3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_excel(\"/home/aift-ml/workspace/lm/seoul_faq_train/finetune/data/seoul/seoul_faq_0503_train.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df = pd.read_excel(\"/home/aift-ml/workspace/lm/seoul_faq_train/finetune/data/seoul/seoul_faq_0503_dev.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "437\n"
     ]
    }
   ],
   "source": [
    "total_label_list = list()\n",
    "for i in train_df.values:\n",
    "    sentence = i[0]\n",
    "    label = str(i[1])\n",
    "    label_kor = str(i[2])\n",
    "    if label_kor not in total_label_list:\n",
    "        total_label_list.append(label_kor)\n",
    "        \n",
    "print(len(total_label_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = total_label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentence= list()\n",
    "train_labels = list()\n",
    "for i in train_df.values:\n",
    "    sentence = i[0]\n",
    "    class_label = str(i[2])\n",
    "    label = [class_labels.index(class_label)]\n",
    "    train_sentence.append(sentence)\n",
    "    train_labels.append(label)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "랜덤하게 질문 합치는 복합 질문 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list = list()\n",
    "for idx in range(len(train_sentence)):\n",
    "    index_list.append(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_train_sentence = []\n",
    "sum_train_labels = []\n",
    "\n",
    "for idx in range(len(train_sentence)):\n",
    "    indexs = random.choices(index_list,k=2)\n",
    "    if train_labels[indexs[0]] != train_labels[indexs[1]]:\n",
    "        new_sentence = f\"{train_sentence[indexs[0]]} {train_sentence[indexs[1]]}\"\n",
    "        new_label = [train_labels[indexs[0]][0], train_labels[indexs[1]][0]]\n",
    "        sum_train_sentence.append(new_sentence)\n",
    "        sum_train_labels.append(new_label)\n",
    "sum_train_sentence.extend(train_sentence)\n",
    "sum_train_labels.extend(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_index_list = list()\n",
    "dev_sentence= list()\n",
    "dev_labels = list()\n",
    "for idx, i in enumerate(dev_df.values):\n",
    "    dev_index_list.append(idx)\n",
    "    sentence = i[0]\n",
    "    class_label = str(i[2])\n",
    "    label = [class_labels.index(class_label)]\n",
    "    dev_sentence.append(sentence)\n",
    "    dev_labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_dev_sentence = []\n",
    "sum_dev_labels = []\n",
    "\n",
    "for idx in range(len(dev_sentence)):\n",
    "    indexs = random.choices(dev_index_list,k=2)\n",
    "    if dev_labels[indexs[0]] != dev_labels[indexs[1]]:\n",
    "        new_sentence = f\"{dev_sentence[indexs[0]]} {dev_sentence[indexs[1]]}\"\n",
    "        new_label = [dev_labels[indexs[0]][0], dev_labels[indexs[1]][0]]\n",
    "        sum_dev_sentence.append(new_sentence)\n",
    "        sum_dev_labels.append(new_label)\n",
    "        \n",
    "sum_dev_sentence.extend(dev_sentence)\n",
    "sum_dev_labels.extend(dev_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset 으로 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_label = ClassLabel(names=total_label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = {\n",
    "    'ID': [str(i) for i in range(len(sum_train_sentence))],\n",
    "    'text': [s for s in sum_train_sentence],\n",
    "    'labels': [l for l in sum_train_labels]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data  = {\n",
    "    'ID': [i for i in range(len(sum_dev_sentence))],\n",
    "    'text': [s for s in sum_dev_sentence],\n",
    "    'labels': [l for l in sum_dev_labels]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train_data)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "validation_df = pd.DataFrame(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = Features({\n",
    "    'ID': Value(dtype='string', id=None),\n",
    "    'text': Value(dtype='string', id=None),\n",
    "    'labels': Sequence(feature=class_label)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(train_df, features=features)\n",
    "test_dataset = Dataset.from_pandas(test_df, features=features)\n",
    "validation_dataset = Dataset.from_pandas(validation_df, features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset,\n",
    "    'validation': validation_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_labels shape ::: (25026, 437)\n",
      "test_labels shape :::: (5239, 437)\n",
      "val_labels shape ::::: (5239, 437)\n",
      "\n",
      "cool..!!\n"
     ]
    }
   ],
   "source": [
    "## convert the integer labels into multi-hot form (44-dimensional).\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "train_labels = mlb.fit_transform(dataset[\"train\"][\"labels\"])\n",
    "test_labels = mlb.fit_transform(dataset[\"test\"][\"labels\"])\n",
    "val_labels = mlb.fit_transform(dataset[\"validation\"][\"labels\"])\n",
    "\n",
    "print(\"train_labels shape ::: {}\".format(train_labels.shape))\n",
    "print(\"test_labels shape :::: {}\".format(test_labels.shape))\n",
    "print(\"val_labels shape ::::: {}\".format(val_labels.shape))\n",
    "print(\"\\ncool..!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## extract the texts, since we will use a custom datset not the huggingface dataset.\n",
    "\n",
    "train_texts = dataset[\"train\"][\"text\"]\n",
    "test_texts = dataset[\"test\"][\"text\"]\n",
    "val_texts = dataset[\"validation\"][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## and the label names...\n",
    "\n",
    "LABELS = dataset[\"train\"].features[\"labels\"].feature.names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## download the pretrained tokenizer from huggingface.\n",
    "\n",
    "MODEL_NAME = \"klue/roberta-base\" \n",
    "# MODEL_NAME = 'beomi/KcELECTRA-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_masking(encoding, prob):\n",
    "    for i, token in enumerate(encoding[\"input_ids\"][0]):\n",
    "        if token not in [0,1,2,3]: # 0 ~ 3, [PAD], [UNK], [CLS], and [SEP], respectively.\n",
    "            if np.random.uniform(0,1) < prob:\n",
    "                encoding[\"input_ids\"][0][i] = 4 # 4 is [MASK]\n",
    "                \n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_switching(encoding, prob):\n",
    "    for i, token in enumerate(encoding[\"input_ids\"][0]):\n",
    "        if token not in [0,1,2,3,4]: # 0 ~ 4, [PAD], [UNK], [CLS], [SEP], and [MASK], respectively.\n",
    "            if np.random.uniform(0,1) < prob:\n",
    "                encoding[\"input_ids\"][0][i] = np.random.choice(np.arange(5,tokenizer.vocab_size), 1)[0]\n",
    "                \n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_and_switch(encoding, prob=0.1):\n",
    "    encoding = token_masking(encoding, prob/2)\n",
    "    encoding = token_switching(encoding, prob/2)\n",
    "    \n",
    "    return encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## maximum token lengths\n",
    "\n",
    "MAX_LENGTH = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define our dataset...!\n",
    "\n",
    "class KOTEDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, texts, labels, tokenizer, max_length:int=MAX_LENGTH,\n",
    "                would_you_like_some_mask_and_switch:bool=False):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.mask = would_you_like_some_mask_and_switch\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx:int):\n",
    "        text = self.texts[idx]\n",
    "        labels = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "            return_token_type_ids=False,\n",
    "        )\n",
    "        \n",
    "        if self.mask:\n",
    "            encoding = mask_and_switch(encoding, prob=0.1)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        return dict(\n",
    "          input_ids=encoding[\"input_ids\"].flatten(),\n",
    "          attention_mask=encoding[\"attention_mask\"].flatten(),\n",
    "          labels=torch.FloatTensor(labels), ## must be a float tensor.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create the datasets.\n",
    "\n",
    "train_dataset = KOTEDataset(train_texts, train_labels, tokenizer=tokenizer, would_you_like_some_mask_and_switch=True)\n",
    "test_dataset = KOTEDataset(test_texts, test_labels, tokenizer=tokenizer)\n",
    "val_dataset = KOTEDataset(val_texts, val_labels, tokenizer=tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "## download the pretrained electra model.\n",
    "\n",
    "electra= AutoModel.from_pretrained(MODEL_NAME, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaConfig {\n",
       "  \"_name_or_path\": \"klue/roberta-base\",\n",
       "  \"architectures\": [\n",
       "    \"RobertaForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_position_embeddings\": 514,\n",
       "  \"model_type\": \"roberta\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"tokenizer_class\": \"BertTokenizer\",\n",
       "  \"transformers_version\": \"4.21.0\",\n",
       "  \"type_vocab_size\": 1,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "electra.config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataloader with pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KOTEDataModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self, train_dataset, test_dataset, val_dataset, batch_size=16):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.train_dataset = train_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=6, ## choose a befitting number depending on your environment.\n",
    "        )\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=6, ## choose a befitting number depending on your environment.\n",
    "        )\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=6, ## choose a befitting number depending on your environment.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16 ## about 28 ~ 30 Gb memory required, if my memory serves me right.\n",
    "\n",
    "data_module = KOTEDataModule(\n",
    "  train_dataset,\n",
    "  test_dataset,\n",
    "  val_dataset,\n",
    "  batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL_LR = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2, alpha=None, size_average=True, device='cpu'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        \"\"\"\n",
    "        gamma(int) : focusing parameter.\n",
    "        alpha(list) : alpha-balanced term.\n",
    "        size_average(bool) : whether to apply reduction to the output.\n",
    "        \"\"\"\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.size_average = size_average\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        # input : N * C (btach_size, num_class)\n",
    "        # target : N (batch_size)\n",
    "\n",
    "        CE = F.cross_entropy(input, target, reduction='none')  # -log(pt)\n",
    "        pt = torch.exp(-CE)  # pt\n",
    "        loss = (1 - pt) ** self.gamma * CE  # -(1-pt)^rlog(pt)\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            alpha = torch.tensor(self.alpha, dtype=torch.float).to(self.device)\n",
    "            # in case that a minority class is not selected when mini-batch sampling\n",
    "            if len(self.alpha) != len(torch.unique(target)):\n",
    "                temp = torch.zeros(len(self.alpha)).to(self.device)\n",
    "                temp[torch.unique(target)] = alpha.index_select(0, torch.unique(target))\n",
    "                alpha_t = temp.gather(0, target)\n",
    "                loss = alpha_t * loss\n",
    "            else:\n",
    "                alpha_t = alpha.gather(0, target)\n",
    "                loss = alpha_t * loss\n",
    "\n",
    "        if self.size_average:\n",
    "            loss = torch.mean(loss)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KOTETagger(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, n_training_steps=None, n_warmup_steps=None, gamma_for_expLR=None):\n",
    "        super().__init__()\n",
    "        self.electra = electra\n",
    "        self.classifier = nn.Linear(self.electra.config.hidden_size, 437) ## the label dimension == 44 <-- what an ominous number for asians though... <-- I didn't intend it!\n",
    "        self.n_training_steps = n_training_steps\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        \n",
    "        ## the loss\n",
    "        # self.criterion = nn.BCELoss()\n",
    "        # self.criterion = nn.CrossEntropyLoss()\n",
    "        self.criterion = FocalLoss()\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        output = self.electra(input_ids, attention_mask=attention_mask)\n",
    "        output = output.last_hidden_state[:,0,:] ## [CLS] of the last hidden state\n",
    "        output = self.classifier(output)\n",
    "        output = torch.sigmoid(output)\n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(output, labels)\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return loss, output\n",
    "    \n",
    "    def step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self.forward(input_ids, attention_mask, labels)\n",
    "\n",
    "        preds = outputs\n",
    "\n",
    "        y_true = list(labels.detach().cpu())\n",
    "        y_pred = list(preds.detach().cpu())\n",
    "\n",
    "        return {\"loss\": loss, \"y_true\": y_true, \"y_pred\": y_pred}\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.step(batch, batch_idx)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.step(batch, batch_idx)\n",
    "    \n",
    "    def epoch_end(self, outputs, state=\"train\"):\n",
    "        loss = torch.tensor(0, dtype=torch.float)\n",
    "        for out in outputs:\n",
    "            loss += out[\"loss\"].detach().cpu()\n",
    "        loss = loss / len(outputs)\n",
    "\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        for out in outputs:\n",
    "            y_true += out[\"y_true\"]\n",
    "            y_pred += out[\"y_pred\"]\n",
    "\n",
    "        self.log(state + \"_loss\", float(loss), on_epoch=True, prog_bar=True)\n",
    "        print(f\"[Epoch {self.trainer.current_epoch} {state.upper()}] Loss: {loss}\")\n",
    "        return {\"loss\": loss}\n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        self.epoch_end(outputs, state=\"train\")\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        self.epoch_end(outputs, state=\"val\")\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=INITIAL_LR)\n",
    "        \n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.n_warmup_steps,\n",
    "            num_training_steps=self.n_training_steps\n",
    "        )\n",
    "        \n",
    "        return dict(\n",
    "          optimizer=optimizer,\n",
    "          lr_scheduler=dict(\n",
    "            scheduler=scheduler,\n",
    "            interval=\"step\"\n",
    "          )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6256, 31280)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## determine the schedule for our optimizer\n",
    "\n",
    "N_EPOCHS = 20\n",
    "\n",
    "steps_per_epoch = len(train_dataset) // BATCH_SIZE\n",
    "TOTAL_STEPS = steps_per_epoch * N_EPOCHS\n",
    "WARMUP_STEPS = TOTAL_STEPS // 5\n",
    "WARMUP_STEPS, TOTAL_STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define the model.\n",
    "\n",
    "model = KOTETagger(\n",
    "    n_warmup_steps=WARMUP_STEPS,\n",
    "    n_training_steps=TOTAL_STEPS,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set a logger and some stuffs...\n",
    "\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "## the check point\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"./multi_output_roberta_focal_3\",\n",
    "    filename=\"epoch{epoch}-val_loss{val_loss:.4f}\",\n",
    "    monitor=\"val_loss\",\n",
    "    save_top_k=3,\n",
    "    mode=\"min\",\n",
    "    auto_insert_metric_name=False,\n",
    ")\n",
    "\n",
    "## for early stopping\n",
    "early_stopping_callback = EarlyStopping(monitor=\"val_loss\", patience=5, min_delta=0.00)\n",
    "\n",
    "## the logger\n",
    "logger = TensorBoardLogger(\"./multi_output_roberta_focal_3\", name=\"multi_output_roberta_focal_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "## trainer\n",
    "\n",
    "N_EPOCHS = 25 ## redefine the number of the epochs, just to make sure there is no more room to improve.\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    logger=logger,\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback],\n",
    "    max_epochs=N_EPOCHS,\n",
    "    gpus=[0], ## GPU number\n",
    "    progress_bar_refresh_rate=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/aift-ml/.local/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name       | Type         | Params\n",
      "--------------------------------------------\n",
      "0 | electra    | RobertaModel | 110 M \n",
      "1 | classifier | Linear       | 336 K \n",
      "2 | criterion  | FocalLoss    | 0     \n",
      "--------------------------------------------\n",
      "110 M     Trainable params\n",
      "0         Non-trainable params\n",
      "110 M     Total params\n",
      "443.817   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s][Epoch 0 VAL] Loss: 12.14773941040039\n",
      "Epoch 0: 100%|█████████▉| 1885/1893 [02:44<00:00, 11.50it/s, loss=8.28, v_num=0][Epoch 0 VAL] Loss: 8.402145385742188\n",
      "Epoch 0: 100%|██████████| 1893/1893 [02:44<00:00, 11.51it/s, loss=8.28, v_num=0, val_loss=8.400][Epoch 0 TRAIN] Loss: 8.797459602355957\n",
      "Epoch 1: 100%|█████████▉| 1890/1893 [02:47<00:00, 11.26it/s, loss=7.86, v_num=0, val_loss=8.400, train_loss=8.800][Epoch 1 VAL] Loss: 7.919424533843994\n",
      "Epoch 1: 100%|██████████| 1893/1893 [02:48<00:00, 11.25it/s, loss=7.86, v_num=0, val_loss=7.920, train_loss=8.800][Epoch 1 TRAIN] Loss: 8.226031303405762\n",
      "Epoch 2: 100%|█████████▉| 1890/1893 [02:46<00:00, 11.33it/s, loss=7.81, v_num=0, val_loss=7.920, train_loss=8.230][Epoch 2 VAL] Loss: 7.723689556121826\n",
      "Epoch 2: 100%|██████████| 1893/1893 [02:47<00:00, 11.33it/s, loss=7.81, v_num=0, val_loss=7.720, train_loss=8.230][Epoch 2 TRAIN] Loss: 7.852303981781006\n",
      "Epoch 3: 100%|█████████▉| 1890/1893 [02:47<00:00, 11.29it/s, loss=7.74, v_num=0, val_loss=7.720, train_loss=7.850][Epoch 3 VAL] Loss: 7.679539203643799\n",
      "Epoch 3: 100%|██████████| 1893/1893 [02:47<00:00, 11.28it/s, loss=7.74, v_num=0, val_loss=7.680, train_loss=7.850][Epoch 3 TRAIN] Loss: 7.725846290588379\n",
      "Epoch 4: 100%|█████████▉| 1890/1893 [02:47<00:00, 11.29it/s, loss=7.57, v_num=0, val_loss=7.680, train_loss=7.730][Epoch 4 VAL] Loss: 7.6443915367126465\n",
      "Epoch 4: 100%|██████████| 1893/1893 [02:47<00:00, 11.28it/s, loss=7.57, v_num=0, val_loss=7.640, train_loss=7.730][Epoch 4 TRAIN] Loss: 7.685352325439453\n",
      "Epoch 5: 100%|█████████▉| 1890/1893 [02:47<00:00, 11.30it/s, loss=7.5, v_num=0, val_loss=7.640, train_loss=7.690] [Epoch 5 VAL] Loss: 7.6357855796813965\n",
      "Epoch 5: 100%|██████████| 1893/1893 [02:47<00:00, 11.30it/s, loss=7.5, v_num=0, val_loss=7.640, train_loss=7.690][Epoch 5 TRAIN] Loss: 7.665043830871582\n",
      "Epoch 6:  10%|█         | 195/1893 [00:20<02:53,  9.79it/s, loss=7.85, v_num=0, val_loss=7.640, train_loss=7.670]"
     ]
    }
   ],
   "source": [
    "## about 4 ~ 5 hours to reach the optimum...\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/aift-ml/workspace/lm/seoul_faq_train/finetune/multi_output_roberta_focal_2/epoch21-val_loss10.2073.ckpt'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "par_dir = '/home/aift-ml/workspace/lm/seoul_faq_train/finetune/multi_output_roberta_focal_2/'\n",
    "best_ckpt = sorted(glob(par_dir + '*.ckpt'))[-1]\n",
    "best_ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "gruesome_mind_reader = KOTETagger.load_from_checkpoint(best_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "gruesome_mind_reader.eval()\n",
    "gruesome_mind_reader.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_model_inference(text):\n",
    "  THRESHOLD = 0.6\n",
    "  encoding = tokenizer.encode_plus(\n",
    "    text,\n",
    "    add_special_tokens=True,\n",
    "    max_length=128,\n",
    "    return_token_type_ids=False,\n",
    "    padding=\"max_length\",\n",
    "    return_attention_mask=True,\n",
    "    return_tensors=\"pt\",\n",
    "  )\n",
    "\n",
    "  _, predictions = gruesome_mind_reader(encoding[\"input_ids\"], encoding[\"attention_mask\"])\n",
    "  predictions = predictions.flatten().numpy()\n",
    "  results = list()\n",
    "  for l,p in zip(LABELS, predictions):\n",
    "      if p < THRESHOLD:\n",
    "          continue\n",
    "      results.append((l,p))\n",
    "  results.sort(key=lambda x:x[1], reverse=True)\n",
    "  for idx, r in enumerate(results):\n",
    "    print(f\"{idx} {r[0]}: {r[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 사망자수: 0.9998960494995117\n",
      "1 한국교육학술정보원_학교알리미 학교정보: 0.9998233914375305\n",
      "2 서울시 학교 기본정보: 0.8325072526931763\n",
      "3 사망원인별 사망자수: 0.787848711013794\n"
     ]
    }
   ],
   "source": [
    "multi_model_inference(\"사망자 수는 몇 명인가? 그리고 초등학교는 몇 개??\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 성/연령별 실업률: 0.9999929666519165\n",
      "1 성/연령별 고용률: 0.9989653825759888\n"
     ]
    }
   ],
   "source": [
    "multi_model_inference(\"고용률 알려줄 수 있나요.. 그리고 최근 실업률도요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 사망원인별 사망자수: 0.9999836683273315\n",
      "1 사망원인별 사망률: 0.9999371767044067\n"
     ]
    }
   ],
   "source": [
    "multi_model_inference(\"백혈병으로 죽는 사람 수와 사망률 궁금\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 유치원 신입원아 수: 0.9999632835388184\n",
      "1 유치원 학급수: 0.998214602470398\n"
     ]
    }
   ],
   "source": [
    "multi_model_inference(\"현재 유치원 학급 수와 유치원생 신입 원아수가 얼마나 되는지 궁금\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 서울시 전동킥보드 견인 현황: 0.9996391534805298\n",
      "1 서울시 전기차 급속충전기 정보 현황: 0.9996383190155029\n"
     ]
    }
   ],
   "source": [
    "multi_model_inference(\"마포구에 전기차 급속충전하는 곳과 전동킥보드 견인 현황은 어디서 보나요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 서울시 정류장별 사용자유형별 환승 승객수 및 환승시간 정보: 0.9999268054962158\n",
      "1 서울시 제설함 위치정보: 0.9994366765022278\n"
     ]
    }
   ],
   "source": [
    "multi_model_inference(\"정류장별로 환승하는 사람 몇 명인지와 제설함 어디에 있는지 알고 싶어요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 고령인구: 0.9999990463256836\n",
      "1 외국인고령자: 0.9999812841415405\n",
      "2 내국인고령자: 0.9985413551330566\n",
      "3 고령인구 구성비: 0.9748948216438293\n",
      "4 서울시 노인(65세이상) 고용지표 통계: 0.9335790276527405\n",
      "5 주민등록인구: 0.7977495789527893\n",
      "6 성/연령별 인구: 0.6776670217514038\n"
     ]
    }
   ],
   "source": [
    "multi_model_inference(\"노인 인구 몇 명이야?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 한국교통안전공단_버스정보시스템(BIS)_버스정류소정보: 0.9984763264656067\n",
      "1 고령인구: 0.9920511841773987\n",
      "2 외국인고령자: 0.9908663630485535\n",
      "3 서울시 버스정류소 위치정보: 0.9883832335472107\n",
      "4 한국교육학술정보원_학교알리미 학교정보: 0.9519776105880737\n",
      "5 내국인고령자: 0.8971028923988342\n"
     ]
    }
   ],
   "source": [
    "multi_model_inference(\"서울시 노인인구 수와 초등학교 수 그리고 버스정류장 수가 궁금합니다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 성/연령/교육정도별 인구: 0.9999997615814209\n",
      "1 한국교육학술정보원_학교알리미 학교정보: 0.9999719858169556\n"
     ]
    }
   ],
   "source": [
    "multi_model_inference(\"대학생은 또 몇 명인지 알려줘요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "97cc609b13305c559618ec78a438abc56230b9381f827f22d070313b9a1f3777"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
